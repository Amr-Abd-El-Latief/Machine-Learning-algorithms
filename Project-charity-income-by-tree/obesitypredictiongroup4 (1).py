# -*- coding: utf-8 -*-
"""ObesityPredictionGroup4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iUIPzy7TLchuRPnM1RIPWA-M5aw9AHPK

Bishoy sameh,
Shireen Talaat,
Amr Abd El Latief,
Ahmed Mahmoud Yassin,
Alsawy Mohamed
"""



"""#Review These
Overview on Decision Trees: https://www.datacamp.com/community/tutorials/decision-tree-classification-python

More on Encoding: https://pbpython.com/categorical-encoding.html
"""

from sklearn.tree import DecisionTreeClassifier
import pandas as pd
import numpy as np

# Run this cell to download the dataset
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00544/ObesityDataSet_raw_and_data_sinthetic%20(2).zip"
!unzip "/content/ObesityDataSet_raw_and_data_sinthetic (2).zip"

#import dataset as a pandas dataframe - name it df - using the .read_csv method
dataset = 'ObesityDataSet_raw_and_data_sinthetic.csv'
df = pd.read_csv(dataset)

#view first 5 rows of the dataset using the .head() method
df.head()

#check for the total amount of null values in each column using the .isnull() method
df.isnull().sum()

"""Number of Columns"""

df.shape[1]

"""Are there any new features that could be derived from our old features?

"""

#create a function that takes your df as an input then calculates the BMI using the equation: BMI= Weight/ Height**2
df['BMI'] = (df.Weight/df.Height**2)
df.head()

# apply the function to every row in the dataframe, create a new column, 'BMI' that contains those values. HINT: You can use .apply and lambda

#check your dataframe

"""Drop old features, i.e: height and weight"""

#use .drop() method to your dataframe, don't forget to set inplace=True
df.drop(columns=['Height', 'Weight'],inplace=True)

#check your dataframe
df.head()

#check your columns' data types using the .dtypes method
df.dtypes

"""Check the unique values in your target label"""

df['NObeyesdad'].unique()

"""Create two Dataframes of Features X and Output Y"""

X = df.drop(['NObeyesdad'], axis=1)
X.head()

y = df['NObeyesdad']
y = y.to_frame()

"""Change column name to something that makes sense"""

#replace newcolumname
y.rename(columns= {'NObeyesdad': 'body_types'}, inplace= True)
y.head()

"""Encode your Catgorical Features"""

from sklearn.preprocessing import OrdinalEncoder

X_encoded = pd.get_dummies(X)
X_encoded =X_encoded.fillna(0.0)
X_encoded.head()

from sklearn.preprocessing import LabelEncoder 
#initialize label encoder
le = LabelEncoder()
# fit the data to your initialized label encoder
y.body_types = le.fit_transform(y['body_types'])
le.classes_

y.head()

"""*Perform* label encoding to y"""

# run this cell to check your label column - y
y.columns

y.head()

"""# New Section"""

#check unique values for y
y['body_types'].unique()

"""Normalize so that no feature has more weight over the other, you can use MinMaxScaler"""

#import scaler - you can use MinMaxScaler
from sklearn.preprocessing import MinMaxScaler

# Initialize a scaler, then apply it to the features
scaler = MinMaxScaler()
# Create a list of your numerical features
numerical = ['Age','FCVC','NCP','CH2O','FAF','TUE','BMI']

X_encoded[numerical] = scaler.fit_transform(X_encoded[numerical])

# Show an example of a record with scaling applied
display(X_encoded.head(n = 1))

"""Shuffle and split the data using train_test_split"""

from sklearn.model_selection import train_test_split

# Split the 'features' and 'income' data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# Show the results of the split
print ("Training set has {} samples.".format(X_train.shape[0]))
print ("Testing set has {} samples.".format(X_test.shape[0]))

"""Training, Prediction and Evaluation

"""

# Let's use Decision Tree Classifier

# import Decision Tree Classifier from Sklearn
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
# initilize the classifier
classifier = DecisionTreeClassifier()

# fit your classifier to the data using the .fit() method
classifier.fit(X_train,y_train)
# predict train set values
y_pred_train = classifier.predict(X_train)

# predict test set values
y_pred_test = classifier.predict(X_test)

# import accuracy_score from sklearn
# check accuracy on training set
acc_train = accuracy_score(y_pred_train,y_train)
# check accuracy on testing set
acc = accuracy_score(y_pred_test,y_test)

print(acc_train)

print(acc)

import sklearn.tree as tree
# visualize your tree
tree.plot_tree(classifier)

# check the hyperparameters used
classifier.get_params(deep=True)

"""**Awesome progress if you've come this far! **

Extra credit - Why don't we play with the hyperparameters we discussed earlier to see how our model changes?

check the documentation and choose the hyperparameters you like: 
https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier
"""

# check your tree's depth
classifier.get_depth()

# Let's use Decision Tree Classifier

# import Decision Tree Classifier from Sklearn
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
# initilize the classifier
classifier2 = DecisionTreeClassifier(min_samples_split=4,max_depth = 6, min_samples_leaf = 5 )

# fit your classifier to the data using the .fit() method
classifier2.fit(X_train,y_train)
# predict train set values
y_pred_train = classifier2.predict(X_train)

# predict test set values
y_pred_test = classifier2.predict(X_test)

# import accuracy_score from sklearn
# check accuracy on training set
acc_train = accuracy_score(y_pred_train,y_train)
# check accuracy on testing set
acc = accuracy_score(y_pred_test,y_test)

print(acc_train)

print(acc)

tree.plot_tree(classifier2)

## fit your classifier to the data using the .fit() method
# predict train set values
# predict test set values

# initialize your classifier again but this time with the hyperparamers you want

# check accuracy on training set
# check accuracy on testing set



# check the hyperparameters used
classifier2.get_params(deep=True)

# check the hyperparameters used
classifier2.get_params(deep=True)

"""Reflect on your results - let's discuss!"""

parameters = { 'min_samples_split': [2], 'max_depth' : [3,4,5,6,7,8,9,10,11,12,13,14,15], 'min_samples_leaf' : [1] }

from sklearn.model_selection import GridSearchCV

best_classifier = GridSearchCV(classifier2, parameters)

# fit your classifier to the data using the .fit() method
best_classifier.fit(X_train,y_train)
# predict train set values
y_pred_train = best_classifier.predict(X_train)

# predict test set values
y_pred_test = best_classifier.predict(X_test)

# import accuracy_score from sklearn
# check accuracy on training set
acc_train = accuracy_score(y_pred_train,y_train)
# check accuracy on testing set
acc = accuracy_score(y_pred_test,y_test)

print(acc_train)

print(acc)

# check the hyperparameters used
best_classifier.get_params(deep=True)



